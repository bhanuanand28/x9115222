# x9115222

# Repository for CS 591(Automated Software Engineering)

Contributors:-

  Bhanu Anand(bhanuanand28)
  
  Esha Sharma(eshasharma)
  
  Vinay Todalge (vntodalge)

_____________________________________________________________________________________________________________________________

##Hyper Parameter Optimization

### Running Instructions 
  1. Clone the github repository x9115222 from git@github.com:bhanuanand28/x9115222.git
  2. Navigate to ./x9115222/hw/code/10 
  3. run testGA.py
  
###Abstract
In this study we used Differential Evolution(DE) to generate the set of values which decide the extend of mutation ,crossover , 
the number of candidates and the number if generations that a genetic algorithm(GA) will run. We use DE to tune our default control 
settings. The DE tunes the GA so that the set of values which performs the most efficiently optimized solution for different 
DTLZs get generated. We apply DE to tune our GAs which optimise DTLZ 1 , 3, 5 , 7 each time with 2,4,6,8 objectives and 10, 20 
, 40 decision. We observe that tuning the GA via DE improves the performance of GA. 

###Introduction and Background
When we use Genetic Algorithm to optimise a problem ,there are a number of parameters which decide the extent of mutation ,
crossover etc. These parameters greatly influence the performance of a GA . In the previous code we used default values for this 
optimisation. In this study we apply Differential Evolution and generate different sets of default parameter settings. These are 
then used to run GA on DTLZ 1 , 3, 5 , 7 each time with 2,4,6,8 objectives and 10, 20 , 40 decisions. We analuze the performance 
of DE on the GAs by comparing the hypervolumes generated before and after the tuning is done.

###Genetic Algorithm 
A Genetic Algorithm is a optimization algorithm which mimics the process of natural selection. In a genetic algorithm we use selection 
to generate the best population, we then use mutationa and crosssover with the default probabilty to generate children. The children
are then compared to the parents to see if the population is getting evolved for better. Like natural selection , this process uses
crossover, mutation and selection. 

###Differential Evolution 
Differential evolution (DE) is a method that optimizes a problem by iteratively trying to improve a candidate solution with regard to 
a given measure of quality. DE optimizes a problem by maintaining a population of candidate solutions and creating new candidate 
solutions by combining existing ones according to its simple formulae, and then keeping whichever candidate solution has the best 
score or fitness on the optimization problem at hand. In this way the optimization problem is treated as a black box that merely 
provides a measure of quality given a candidate solution and the gradient is therefore not needed.

###Implementation 
To tune the GA via DE , we passed an object of GA to DE. The candidate function of the GA created a frontier of parameter lists. The 
DE tuned the GA for the parameter lists passed. 
self.hi=[300, 500, 0.99, 0.09, 0.9]
        self.lo=[100, 100, 0.50, 0.01, 0.4]

For each set of default parameters passed, the GA ran on the model passed and generated the final pareto frontier. This final pareto 
frontier was compared to the initial pareto frontier and the loss value was calculated and returned. On each iteration of the DE, the 
default parameter list created was evaluated using this loss value. If there was an improvement in the loss value it means that there
was a significant improvement from the first to the last iteration of the GA and hence the DE was doing a good job with the optimization.
Also , the GA was run on DTLZ 1 , 3, 5 , 7 each time with 2,4,6,8 objectives and 10, 20 , 40 decisions Every time the DE optimized the 
GA for each of these models , it calculated the hypervolume for the last frontier generated by the GA. 

To check whether or not the DE tunes the GA well , we stored the hypervolumes for each run of GA on DTLZ 1 , 3, 5 , 7 each time with 
2,4,6,8 objectives and 10, 20 , 40 decisions and plotted this on a graph. 

We also ran the untuned GAs which we had generated in Code 9 and compared the graphs so as to compar eth performance before and after 
the GA was applied on.
The paramters for this run were :  
kmax = 200 , 
population size = 100 
lives = 5
crossover =0.98
mutation_proba=0.05 
frontier_distributon - 0.8



###Results

####DTLZ1 - before and after optimization

![alt tag](https://github.com/bhanuanand28/x9115222/blob/master/hw/code/10/ScreenShots/DTLZ1.jpg)

####DTLZ3 - before and after optimization

![alt tag](https://github.com/bhanuanand28/x9115222/blob/master/hw/code/10/ScreenShots/DTLZ3.jpg)

####DTLZ5 - before and after optimization

![alt tag](https://github.com/bhanuanand28/x9115222/blob/master/hw/code/10/ScreenShots/DTLZ5.jpg)

####DTLZ7 - before and after optimization

![alt tag](https://github.com/bhanuanand28/x9115222/blob/master/hw/code/10/ScreenShots/DTLZ7.jpg)
It can be observed from the graphs that GA wokrs best on

###Conclusions

###Threats to Validity 
1. We ran the code only for 20 iterations . Running the code for a larger number of iterations may produce better statitics. 
2. We used the Hypervolume between generations to compute the graph . We could have used multiple other tools to calcuate the 
  efficiency as well . 
3. We checked only for DTLZ . This could have been checked for multiple models . 

###Future Work 
1. The code could be run for more number of iterations. 
2. We could use hypervolume etc to generate more than 20 graphs and affirm the results across these graphs . 
3. We could use functions other than hypervolume etc to compare efficiency. 
4. We could vary the early termination conditions . 
5. We could use GA to tune models other than DTLZ and check performance on those . 

###References:-

 1. [Pseudo-code for Genetic Algorithm](http://www.cleveralgorithms.com/nature-inspired/evolution/genetic_algorithm.html)
 2. Book : Clever Algorithms by Jason Brownlee
 3. https://github.com/txt/mase/blob/master/lessthan.md
 4. https://github.com/txt/mase/blob/master/STATS.md
 5. https://en.wikipedia.org/wiki/Genetic_algorithm


###Acknowledgements

   The study uses code found here :
 1.  This study uses code for Scott Knott given here : https://github.com/txt/mase/blob/master/src/doc/sk.py
 2.  This study used Hypervolume functions given here: 
     [Hypervolume Calculator](https://github.com/ai-se/storm/tree/master/PerformanceMetrics) 


